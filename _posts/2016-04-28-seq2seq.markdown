---
layout:     post
title:      "seq2seq: Sequence to Sequence Learning"
subtitle:   ""
date:       2016-04-25 14:30:00
author:     "Beld"
header-img: "img/post-bg-ml.png"
tags:
    - Tensorflow
    - Machine Learning
    - Deep Learning
---

##### General sequence to sequence learning problem with neural networks

The Recurrent Neural Network (RNN) is a natural generalization of feedforward neural networks to sequences. However, it is not clear how to apply an RNN to problems whose input and the output sequences have different lengths with complicated and non-monotonic relationships. The simplest strategy for general sequence learning is to map the input sequence to a fixed-sized vector using one RNN, and then to map the vector to the target sequence with another RNN. But it would be difficult to train the RNNs due to the resulting long term dependencies.

Seq2seq: to use one LSTM to read the input sequence, one timestep at a time, to obtain large fixed-dimensional vector representation, and then to use another LSTM to extract the output sequence from that vector. The second LSTM is essentially a recurrent neural network language model except that it is conditioned on the input sequence.


Note that the LSTM reads the input sentence in reverse, because doing so introduces many short term dependencies in the data that make the optimization problem much easier. Although LSTMs tend to not suffer from the vanishing gradient problem, they can have exploding gradients. So we can enforce a hard constraint on the norm of the gradient.


##### Bucketing and padding
Tensorflow makes use of bucketing to efficiently handle sentences of different lengths despite it is not very convenient.
Since the English sentence is passed as encoder_inputs, and the French sentence comes as decoder_inputs (prefixed by a GO symbol), we should in principle create a seq2seq model for every pair (L1, L2+1) of lengths of an English and French sentence. On the other hand, we could just pad every sentence with a special PAD symbol. Then we'd need only one seq2seq model, for the padded lengths. But on shorter sentence our model would be inefficient, encoding and decoding many PAD symbols that are useless. As a compromise between constructing a graph for every pair of lengths and padding to a single length, we use a number of buckets and pad each sentence to the length of the bucket above it. buckets = [(5, 10), (10, 15), (20, 25), (40, 50)]


In practice, this is done by maximizing the likelihood of each target token given the current state of the model (which summarizes the input and the past output tokens) and the previous target token, which helps the model learn a kind of language model over target tokens. However, during inference, true previous target tokens are unavailable, and are thus replaced by tokens generated by the model itself, yielding a discrepancy between how the model is used at training and inference.

The main problem is that mistakes made early in the sequence generation process are fed as input to the model and can be quickly amplified because the model might be in a part of the state space it has never seen at training time.


Curriculum learning to change the training process in order to gradually force the model to deal with its own mistakes, as it would have to during inference. Doing so, the model explores more during training and is thus more robust to correct its own mistakes at inference as it has learned to do so during training.
