---
layout:     post
title:      "隐马尔科夫模型（二）"
subtitle:   "向前向后算法，维特比算法"
date:       2016-02-03 17:11:00
author:     "Beld"
header-img: "img/post-bg-ml.png"
tags:
    - Machine Learning
---

#### 隐马尔可夫模型的应用

总的来说，隐马尔可夫有四种应用：  
第一，给定一个观测序列$$x_1,x_2,x_3...$$，假定模型参数已知$$\theta=\{\pi,A,\psi\}$$，求给定的观测序列在此模型下实际发生的概率，比如数据的似然概率$$p(X|\theta)$$。这样如果我们有多个模型，就可以选择概率最高的做为最有可能的模型。似然概率可以通过前向算法计算。这就是一个有监督学习问题，可以看作是推断步骤。

在得到数据似然的基础上，我们可以解决两种问题，滤波（Filtering）和平滑（Smoothing)：  
滤波是计算$$p(z_t|x_{1:t})$$，新估计的状态概率只根据之前的观测结果，可以通过前向算法计算。  
平滑是计算$$p(z_t|x_{1:T})$$，状态概率是基于所有的观测，包括未来的，可以通过向前向后算法计算。

给定一个观测序列$$x_1,x_2,x_3...$$，假定模型参数已知$$\theta =\{\pi,A,\psi\}$$，求最佳状态序列$$z_1,z_2,z_3...$$，可以通过维特比算法计算。

给定一个观测序列$$x_1,x_2,x_3...$$，求最佳模型参数$$\theta =\{\pi,A,\psi\}$$。这通常是最难的。可以通过最大期望EM算法计算或者Baum-Welch算法。

#### HMM的最大似然
如果我们观测到一个数据集X = \{x1,...,x_N\}，那么我们可以使用最大似然法确定HMM的参数。似然函数通过对联合概率分布中的潜在变量进行求和的方式得到，即
<center>$$p(X|\theta) = \sum\limits_Zp(X,Z|\theta) $$</center>
由于$$Z$$既不相互独立，又每个都有$$K$$个状态。所以直接求参数是不可行的。其实这个求和对应了晶格图中多条路径的求和。我们可以像在图模型中利用对条件独立性质对求和式进行重新排序，将指数关系转化为线性。

这里还有另一个问题。回想一下，其实独立同分布的混合模型是HMM的一个具体实例，比如独立同分布的GMM。在那里，我们求最大似然MLE时无法得出解析解，我们运用了EM算法。同样，我们也可以用EM算法求解HMM的最大似然。

EM算法的开始阶段是对模型参数的某些初始的选择，我们记作$$θ^{old}$$。在E步骤中，我们使用 这些参数找到潜在变量的后验概率分布$$p(Z|X,θ^{old})$$。然后，我们使用这个后验概率分布计算完整数据似然函数的对数的期望，得到了一个关于参数$$θ$$的函数$$Q(θ,θ^{old})$$，定义为
<center>$$Q(\theta,\theta^{old}) = \sum \limits_{Z} p(Z|X,\theta^{old})\ln p(X,Z | \theta)$$</center>
现在，引入一些记号会比较方便。我们使用$$γ(z_n)$$来表示潜在变量$$z_n$$的后验概率分布， 用$$ξ(z_{n−1},z_n)$$表示两个连续的潜在变量的联合后验概率分布，即
$$
\begin{eqnarray}
\gamma(z_N) = p(z_n|X,\theta^{old}) \\
\xi(z_{n-1},z_n) = p(z_{n-1},z_n|X,\theta^{old})
\end{eqnarray}
$$
对于每个$$n$$值，我们可以使用$$K$$个非负数来存储$$γ(z_n)$$，这些数的和等于$$1$$。类似的，我们可以使用一个由非负数组成的$$K×K$$的矩阵来存储$$ξ(z_{n−1},z_n)$$，同样加和等于$$1$$。我们也会使用$$γ(z_{nk})$$来表示$$z_{nk}=1$$的条件概率，由于二值随机变量的期望就是取值为1的概率，因此得到
$$
\begin{eqnarray}
\gamma(z_{nk}) = \mathbb{E}[z_{nk}] = \sum\limits_{z_n}\gamma(z)z_{nk}\\
\xi(z_{n−1,j},z_{nk}) = \mathbb{E}[z_{n−1,j}z_{nk}] = \sum\limits_{z_{n-1},z_n}\xi(z_{n-1},z_n)z_{n-1,j}z_{nk}
\end{eqnarray}$$

#### 向前向后算法 forward-backward
向前向后算法顾名思义就是分为向前部分和向后部分。我们感兴趣的是在给定观测数据$$x_1,...,x_n$$的条件下，计算$$z_n$$的后验概率分布$$p(z_n|x_1,...,x_n)$$。
<center>$$\gamma(z_n) = p(z_n|X) = \frac{p(X|z_n)p(z_n)}{p(X)}$$</center>
分母p(X)隐式的以HMM的参数$$θ^{old}$$为条件，因此表示似然函数。使用条件独立性质，$$z_n$$阻隔了$$x_1,...,x_n$$与$$x_{n+1},...,x_N$$，所以有
<center>$$\gamma(z_n) = \frac{p(x_1,...,x_n,z_n)p(x_{n+1},...,x_N|z_n)}{p(X)} = \frac{\alpha(z_n)\beta(z_n)}{p(X)}$$</center>
其中我们定义了
\begin{eqnarray}
\alpha(z_n) \equiv p(x_1,...,x_n,z_n)  \\
\beta(z_n) \equiv p(x_{n+1},...,x_N|z_n)
\end{eqnarray}
$$α(z_n)$$就是向前算法部分，表示观测到时刻$$n$$及其之前时刻的所有数据和$$z_n$$的值的联合概率，而$$β(z_n)$$就是向后算法部分，表示在给定$$z_n$$的条件下，从时刻$$n+1$$到$$N$$的所有未来数据的条件概率。

向前算法部分，和Bayes Filter一样，可以推导出递归关系
$$
\begin{eqnarray}
\alpha(z_n) = p(x_1,...,x_n,z_n) \\
= p(x_1,...,x_n|z_n)p(z_n) \\
= p(x_n|z_n)p(x_1,...,x_{n-1}|z_n)p(z_n) \\
= p(x_n|z_n)p(x_1,...,x_{n-1},z_n) \\
= p(x_n|z_n)\sum\limits_{z_{n-1}}p(x_1,...,x_{n-1},z_{n-1},z_n) \\
= p(x_n|z_n)\sum\limits_{z_{n-1}}p(x_1,...,x_{n-1},z_n|z_{n-1})p(z_{n-1}) \\
= p(x_n|z_n)\sum\limits_{z_{n-1}}p(x_1,...,x_{n-1}|z_{n-1})p(z_n|z_{n-1})p(z_{n-1}) \\
= p(x_n|z_n)\sum\limits_{z_{n-1}}p(x_1,...,x_{n-1},z_{n-1})p(z_n|z_{n-1})
\end{eqnarray}$$
<center>$$\alpha(z_n) = p(x_n|z_n)\sum\limits_{z_{n-1}}\alpha(z_{n-1})p(z_n|z_{n-1}) $$</center>
求和式中有$$K$$项，又必须对$$z_n$$的$$K$$个值中的每一个进行计算，因此$$α$$递归的每一步的计算代价为$$O(K^2)$$。

为了开始这个递归过程，我们需要一个初始条件，形式为
<center>$$\alpha(z_1) = p(x_1,z_1) = p(z_1)p(x_1|z_1) = \prod\limits_{k=1}^K\{\pi_kp(x_1|\phi_k)\}^{z_{1k}}$$</center>
从链的第一个结点开始，我们可以沿着链计算每个潜在结点的$$α(z_n)$$。由于递归的每一步涉及到与一个$$K×K$$的矩阵相乘，因此计算整个链的这些量的整体代价是$$O(K^2N)$$。

类似地我们可以推出$$β(z_n)$$的递归关系，即
$$
\begin{eqnarray}
\beta{z_n} &=& p(x_{n+1},...,x_N|z_n) \\
= \sum\limits_{z_{n+1}}p(x_{n+1},...,x_N,z_{n+1}|z_n) \\
= \sum\limits_{z_{n+1}}p(x_{n+1},...,x_N|z_n,z_{n+1})p(z_{n+1}|z_n) \\
= \sum\limits_{z_{n+1}}p(x_{n+1},...,x_N|z_{n+1})p(z_{n+1}|z_n) \\
= \sum\limits_{z_{n+1}}p(x_{n+2},...,x_N|z_{n+1})p(z_{x+1}|z_{n+1})p(z_{n+1}|z_n) \\
\end{eqnarray}$$
<center>$$\beta(z_n) = \beta(z_{n+1})p(x_{n+1}|z_{n+1})p(z_{n+1}|z_n) $$</center>
在这种情况下，我们得到了一个后向信息传递算法，它根据$$β(z_{n+1})$$计算$$β(z_n)$$。在每一步中，我们通过发射概率$$p(x_{n+1}|z_{n+1})$$将观测$$x_{n+1}$$的效果吸收进来，然后对$$z_{n+1}$$求和。下图说明了这个过程。
![](https://mqshen.gitbooks.io/prml/content/Chapter13/hmm/images/13_13.png)
在这个晶格图片段中，我们看到$$β(z_n,1)$$的计算方式是将$$n+1$$步的$$β(z_{n+1})$$的元素$$β(z_{n+1},k)$$加权求和，权值为$$A_{ik}（对应于$$p(z_{n+1}|z_n)$$)与发射概率密度$$p(x_n|z_{n+1},k)$$的对应值的乘积。

与之前一样，我们需要一个递归的起始条件，即$$β(z_n)$$的一个值。
<center>$$p(z_N|X) = \frac{p(X,z_N)\beta(z_N)}{p(X)} $$</center>
由上式可以看出，只要我们对于所有的$$z_n$$都有$$β(z_n)=1$$，这个结果就是正确的。

#### 维特比算法 Viterbi algorithm
通过向前向后算法，在给定观测序列下，我们可以推断某个隐含状态，但是不能推出最有可能的隐含状态序列。在隐马尔可夫模型的许多应用中，潜在变量有许多有意义的直观意义，因此对于给定的观测序列，我们常常感兴趣的是寻找概率最高的隐含状态序列。例如，在语音识别中，对于一个给定的声音观测序列，我们可能希望找到概率最大的音素序列。由于隐马尔可夫模型的图是一棵有向树，因此这个问题可以使用最大加和算法精确地求解。回忆一下在图模型中，寻找潜在变量的概率最高的序列与寻找分别概率最高的状态的集合是不相同的。
![](https://mqshen.gitbooks.io/prml/content/Chapter13/hmm/images/13_16.png)
上图是一个HMM晶格图片段，画出了两条可能的路径。维特比算法从指数多种可能的路径中高效地确定概率最高的路径。对于任意给定的路径，对应的概率为转移矩阵的元素$$A_{jk}$$（对应于每个路径片段的概率$$p(z_{n+1}|z_n)$$）和与路径上的每个结点相关联的发射概率密度$$p(x_n|k)$$的乘积。

与加-乘算法相同，我们首先将隐马尔可夫模型表示为因子图。我们将变量结点zn当成根结点，从根结点开始向叶结点传递信息。我们看到在最大加和算法中传递的信息为
