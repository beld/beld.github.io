---
layout:     post
title:      "Variational Inference 变分推断"
subtitle:   ""
date:       2016-01-23 23:30:00
author:     "Beld"
header-img: "img/post-bg-ml.png"
tags:
    - Machine Learning
---

#### Motivation 前言
在概率模型的应用中，一个核心任务是在给定观测数据变量$$X$$的条件下，计算潜在变量Z的后验概率分布$$p(Z|X)$$，以及计算关于这个后验概率分布的期望。模型可能也包含某些确定性参数，我们现在不考虑它。模型也可能是一个纯粹的贝叶斯模型，其中任何未知的参数都有一个先验概率分布，并且被整合到了潜在变量集合中，记作向量$$Z$$。对于实际应用中的许多模型来说，计算后验概率分布或者计算关于这个后验概率分布的期望是不可行的。这可能是由于潜在空间的维度太高，以至于无法直接计算，或者由于后验概率分布的形式特别复杂，从而期望无法解析地计算。

在这种情况下，我们需要借助近似方法。近似方法大体分为随机近似和确定性近似两大类。随机采样方法，例如马尔科夫链蒙特卡罗方法，这些方法通常具有这样的性质：给定无限多的计算资源，它们可以生成精确的结果，近似的来源是使用了有限的处理时间。在实际应用中，取样方法需要的计算量会相当大，经常将这些方法的应用限制在了小规模的问题中。并且，判断一种取样方法是否生成了服从所需的概率分布的独立样本是很困难的。另一种就是确定性近似方法。有些方法对于大规模的数据很适用。这些方法基于对后验概率分布的解析近似，例如通过假设后验概率分布可以通过一种特定的方式分解，或者假设后验概率分布有一个具体的参数形式，例如高斯分布。对于这种情况，这些方法永远无法生成精确的解，因此这些方法的优点和缺点与取样方法是互补的。

#### Functional 泛函
我们可以将函数想象为一个映射。这个映射以一个变量的值作为输入，返回函数值作为输出。函数的导数描述了当输入变量有一个无限小的变化时，输出值如何变化。类似地，我们可以将泛函（functional）作为一个映射，它以一个函数作为输入，返回泛函的值作为输出。一个例子是熵H[p]，它的输入是一个概率分布p(x)，返回下面的量
H[p] = - \int p(x)\ln p(x) dx \tag{10.1}
作为输出。我们可以引入泛函的导数（functional derivative）的概念，它表达了输入函数产生无穷小的改变时，泛函的值的变化情况。

许多问题可以表示为最优化问题Optimization，其中需要最优化的量是一个泛函，研究所有可能的输入函数，找到最大化或者最小化泛函的函数就是问题的解。虽然变分方法本质上没有任何近似的东西，但是它们通常会被用于寻找近似解。寻找近似解的过程可以这样完成：限制需要最优化算法搜索的函数的范围，例如只考虑二次函数，或考虑由固定的基函数线性组合而成的函数，其中只有线性组合的系数可以发生变化。
