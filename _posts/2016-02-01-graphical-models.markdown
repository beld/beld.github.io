---
layout:     post
title:      "图模型"
subtitle:   "贝叶斯网络，马尔科夫随机场，联合概率分解，条件独立表示，图的概率推断，条件随机场"
date:       2016-02-01 23:00:00
author:     "Beld"
header-img: "img/post-bg-ml.png"
tags:
    - Machine Learning
---



##### 树
在无向图的情形中，树被定义为满足下面性质的图：任意一对结点之间有且只有一条路径。于是这样的图没有环。在有向图的情形中，树的定义为：有一个没有父结点的结点，被称为根（root），其他所有的结点都有一个父结点。如果有向图中存在具有多个父结点的结点，但是在任意两个结点之间仍然只有一条路径（忽略箭头方向），那么这个图被称为多树（polytree）。

##### 因子图 factor graph
有向图和无向图都使得若干个变量的一个全局函数能够表示为这些变量的子集上的因子的乘积，因子图统一地显式地表示出了这个分解，方法是：在表示变量的结点的基础上，引入额外的结点表示因子本身。
<center>$$p(x) = \prod\limits_sf_s(x_s) $$</center>
在因子图中，概率分布中的每个变量都有一个结点（同样用圆圈表示），这与有向图和无向图的情形相同。还存在其他的结点（用小正方形表示），表示联合概率分布中的每个因子$$f_s(x_s)$$。最后，在每个因子结点和因子所依赖的变量结点之间，存在无向链接。由于因子图由两类不同的结点组成，且所有的链接都位于两类不同的结点之间，因此因子图被称为二分的（bipartite）。

同一个有向图或者无向图都可能对应于多个因子图，这使得因子图对于分解的精确形式的表示更加具体。
![](https://mqshen.gitbooks.io/prml/content/Chapter8/inference/images/full_connect.png)
![](https://mqshen.gitbooks.io/prml/content/Chapter8/inference/images/directed_factor_graph.png)

##### 加和-乘积算法 sum-product algorithm
有向无环图的精确推断的置信传播（belief propagation）的算法等价于加-乘算法的一个具体情形。

我们假设模型中所有的变量都是离散的，因此求边缘概率对应于求和的过程。再假设原始的图是一个无向树或者有向树或者多树，从而对应的因子图有一个树结构。

我们可以将原始的图转化为因子图，使得我们可以使用同样的框架处理有向模型和无向模型。

我们的目标是利用图的结构完成两件事：(1)得到一个高效的精确推断算法来寻找边缘概率，(2)在需要求解多个边缘概率的情形，计算可以高效地共享。

首先，因子图用因子结点的乘积表示联合分布。
<center>$$p(x) = \prod\limits_{s }f_s(\mathbb{x_s})$$</center>
对于特定的变量结点$$x$$，我们寻找边缘概率$$p(x)$$。现阶段，我们假设所有的变量都是隐含变量。根据定义，边缘概率分布通过对所有$$x$$之外的变量上的联合概率分布进行求和的方式得到，即
<center>$$p(x) = \sum\limits_{\mathbb{x}\\x}p(\mathbb{x})$$</center>
算法的思想是使用因子图的因子乘积表达式替换$$p(x)$$，然后交换加和与乘积的顺序，得到一个高效的算法。
![](http://i13.tietuku.com/8d89c9673305450b.png)
如图的联合概率分布可以写成乘积的形式
<center>$$p(x) = \prod\limits_{s \in ne(x)}F_s(x,X_s)$$</center>
其中$$ne(x)$$表示与$$x$$相邻的因子结点的集合，$$X_s$$表示子树中通过因子结点$$f_s$$与变量结点$$x$$相连的所有变量的集合，$$F_s(x,X_s)$$表示分组中与因子$$f_s$$相关联的所有因子的乘积。
交换加和与乘积的顺序，我们有
\begin{eqnarray}
p(x) = \prod\limits_{s \in ne(x)}\left[\sum\limits_{X_s}F_s(x,X_s)\right] \\
= \prod\limits_{s \in ne(x)}\mu_{f_s \to x}(x)
\end{eqnarray}
函数$$μ_{f_s}→x(x)$$可以被看做从因子结点$$f_s$$到变量结点$$x$$的信息（message）。我们看到，需要求解的边缘概率分布$$p(x)$$等于所有到达结点$$x$$的输入信息的乘积。

我们注意到每个因子$$F_s(x,X_s)$$由一个因子（子）图，因此本身可以被分解。

算法思想总结：我们可以将结点x看成树的根结点，x的边缘概率分布等于沿着所有到达这个结点的链接的输入信息的乘积。从叶结点开始计算，如果一个叶结点是一个变量结点，那么它沿着与它唯一相连的链接发送的信息为$$1$$。如果叶结点是一个因子结点，发送的信息的形式为$$f(x)$$。每个结点都可以向根结点发送信息。一旦结点收到了所有其他相邻结点的信息，那么它就可以向根结点发送信息。递归地传递信息，直到信息被沿着每一个链接传递完毕，并且根结点收到了所有相邻结点的信息。
![](https://mqshen.gitbooks.io/prml/content/Chapter8/inference/images/sum_product.png)
对于标准化系数的问题。如果因子图是从有向图推导的，那么联合概率分布已经正确的被标准化了，因此通过加-乘算法得到的边缘概率分布会类似的被正确标准化。然而，如果我们开始于一个无向图，那么通常会存在一个未知的标准化系数1/Z。

现在，考虑一个简单的例子：
![](https://mqshen.gitbooks.io/prml/content/Chapter8/inference/images/factor_sum_product.png)
它的未标准化联合概率分布为
<center>$$\tilde{p}(x) = f_a(x_1,x_2)f_b(x_2,x_3)f_c(x_2,x_4)$$</center>
让我们令结点$$x_3$$为根结点，此时有两个叶结点$$x_1$$,$$x_4$$。从叶结点开始，我们有
\begin{eqnarray}
\mu_{x_1 \to f_a}(x_1) = 1  \\
\mu_{f_a \to x_2}(x_2) = \sum\limits_{x_1}f_a(x_1, x_2)  \\
\mu_{x_4 \to f_c}(x_4) = 1 \\
\mu_{f_c \to x_2}(x_2) = \sum\limits_{x_4}f_c(x_2, x_4) \\
\mu_{x_2 \to f_b}(x_2) = \mu_{f_a \to x_2}(x_2)\mu_{f_c \to x_2}(x_2)  \\
\mu_{f_b \to x_3}(x_3) = \sum\limits_{x_2}f_b(x_2, x_3)\mu_{x_2 \to f_b}(x_2)
\end{eqnarray}
一旦信息传播完成，我们就可以将信息从根结点传递到叶结点，这些信息为
\begin{eqnarray}
\mu_{x_3 \to f_b}(x_3) = 1  \\
\mu_{f_b \to x_2}(x_2) = \sum\limits_{x_3}f_b(x_2, x_3) \\
\mu_{x_2 \to f_a}(x_2) = \mu_{f_b \to x_2}(x_2)\mu_{f_c \to x_2}(x_2) \\
\mu_{f_a \to x_1}(x_1) = \sum\limits_{x_2}f_a(x_1, x_2)\mu_{x_2 \to f_a}(x_2) \\
\mu_{x_2 \to f_c}(x_2) = \mu_{f_a \to x_2}(x_2)\mu_{f_b \to x_2}(x_2)  \\
\mu_{f_c \to x_4}(x_4) = \sum\limits_{x_2}f_c(x_2, x_4)\mu_{x_2 \to f_c}(x_2)
\end{eqnarray}
现在一个信息已经在两个方向上通过了每个链接，因此我们现在可以计算边缘概率分布。作为一个简单的检验，让我们验证边缘概率分布$$p(x_2)$$由正确的表达式给出。使用上面的结果将信息替换掉，我们有
\begin{eqnarray}
\tilde{p}(x_2) = \mu_{f_a \to x_2}(x_2)\mu_{f_b \to x_2}(x_2)\mu_{f_c \to x_2}(x_2) \\
= \left[\sum\limits_{x_1}f_a(x_1,x_2)\right]\left[\sum\limits_{x_3}f_b(x_2,x_3)\right]\left[\sum\limits_{x_4}f_c(x_2,x_4)\right] \\
= \sum\limits_{x_1}\sum\limits_{x_3}\sum\limits_{x_4}f_a(x_1,x_2)f_b(x_2,x_3)f_c(x_2,x_4) \\
= \sum\limits_{x_1}\sum\limits_{x_3}\sum\limits_{x_4}\tilde{p}(x)
\end{eqnarray}

##### 最大和算法 max-sum
加乘算法能够将联合概率分布表示为一个因子图，并且高效地求出成分变量上的边缘概率分布。但是怎么找到联合概率分布的最大值以及相应的变量设置呢？

一个想法是用加乘算法找到每个点边缘概率分布，分别求出使得边缘概率最大的变量值。然而，这一组值每个值都单独取得最大的概率，并不能使得联合概率分布具有最大值。

在推导加-乘算法时，我们使用了乘法的分配律。最大化也可以类似进行：
<center>$$\max(ab,ac) = a \max(b,c)  $$</center>
这对于$$a≥0$$的情形成立（这对于图模型的因子总成立）。这使得我们交换乘积与最大化的顺序。
\begin{eqnarray}
\max_x p(x) = \frac{1}{Z}\max_{x_1}\dots\max_{x_M}[\psi_{1,2}(x_1,x_2)\dots\psi_{N-1,N}(x_{N-1},x_N)] \\
=  \frac{1}{Z}\max_{x_1}\left[\max_{x_2}\left[\psi_{1,2}(x_1,x_2)\left[\dots\max_{x_N}\psi_{N-1,N}(x_{N-1},x_N)\right]\dots\right]\right]
\end{eqnarray}
正如边缘概率的计算一样，我们看到交换最大值算符和乘积算法会产生一个更高效的计算，且更容易表示为从结点xN沿着结点链传递回结点$$x_1$$的信息。最后对所有到达根结点的信息的乘积进行最大化，这可以被称为最大化乘积算法（max-produce algorithm），与加-乘算法完全相同唯一的区别是求和被替换为了求最大值。注意，现阶段信息被从叶结点发送到根结点，而没有相反的方向。

在实际应用中，许多小概率的乘积可以产生数值下溢的问题，因此更方便的做法是对联合概率分布的对数进行操作。对数函数是一个单调函数，因此求最大值的运算符可以与取对数的运算交换顺序，因此我们得到了最大化和算法（max-sum algorithm）。

如何寻找联合概率达到最大值的变量的配置呢？找到根节点变量的概率最大值后，将信息从根结点传回叶结点，然而，由于我们现在进行的是最大化过程而不是求和过程，因此有可能存在多个$$x$$的配置，它们都会给出$$p(x)$$的最大值。我们需要确定对应于同样的最大化配置的前一个变量的状态。可以通过跟踪变量的哪个值产生了每个变量的最大值状态，即存储下面的量
<center>$$\phi(x_n) =  \arg\max_{x_{n-1}}\left[\ln f_{n-1,n}(x_{n-1},x_n) + \mu_{x_{n-1} \to f_{n-1, n}(x_{n-1})}\right] $$</center>
对于给定变量的每个状态，存在前一个变量的一个唯一的状态使得概率取最大值。一旦我们知道了最终结点xN的最可能的值，我们就可以沿着链接回退，找到结点$$x_{N−1}$$的最可能状态，并且以此类推，回到最初的结点$$x_1$$。这对应于将信息沿着链进行反方向的传递，使用下面的公式
<center>$$x_{n-1}^{max} = \phi(x_n^{max})$$</center>
被称为反向跟踪（back-tracking）。注意，可能存在多个xn−1的值，每个都能给出最大值。在进行反向跟踪时，只要我们选择了这些变量中的一个，那么我们就能够保证得到一个全局相容的最大化配置。

##### 条件随机场 Conditional Random Fields
条件随机场是一种判别式（discriminative）概率无向图模型，被用来对连续随机变量的特征进行分类，用于标注和切分有序数据的条件概率。

如果给定的马尔可夫随机场MRF中每个随机变量下面还有观察值，我们要确定的是给定观察集合下，这个MRF的分布，也就是条件分布，那么这个MRF就称为CRF。它的条件分布形式完全类似于MRF的分布形式，只不过多了一个观察集合x。因此我们可以认为CRF本质上是给定了观察值(observations)集合的MRF。

可以推导出CRF表示条件概率的公式：

用训练数据计算出使得后验概率最大的参数值，我们采用最小化负对数后验，再转化为似然乘以先验，先验可以用高斯估计。但是似然概率并不可解，我们可以用伪似然pseudo-likelihood去近似。只计算马尔可夫毯Markov blanket。
