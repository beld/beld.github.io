---
layout:     post
title:      "Mixture Models and Expectation Maximization 混合模型及EM"
subtitle:   ""
date:       2016-01-24 23:30:00
author:     "Beld"
header-img: "img/post-bg-ml.png"
tags:
    - Machine Learning
---

#### Motivation 前言
如果我们定义观测变量和潜在变量的一个联合概率分布，那么对应的观测变量本身的概率分布可以通过求边缘概率的方法得到。这使得观测变量上的复杂的边缘概率分布可以通过观测变量与潜在变量组成的扩展空间上的更加便于计算的联合概率分布来表示。因此，潜在变量的引入使得复杂的概率分布可以由简单的分量组成。

除了提供了一个构建更复杂的概率分布的框架之外，混合模型也可以用于数据聚类。一个非概率的聚类方法就是$$K$$均值算法。之后，我们引入混合概率分布的潜在变量观点，其中离散潜在变量可以被看做将数据点分配到了混合概率分布的具体成分当中。潜在变量模型中寻找最大似然估计的一个一般的方法是期望最大化（EM）算法。

高斯混合模型广泛应用于数据挖掘、机器学习和统计分析中。在许多应用中，参数由最大似然方法确定，通常会使用EM算法。然而，最大似然方法会有一些巨大的局限性。如果使用变分推断的方法，可以得到一个优雅的贝叶斯处理方式。

#### $$K－means$$聚类
首先，我们考虑寻找多维空间中数据点的分组或聚类的问题。假设我们有一个数据集$$\{x_1,...,x_N\}$$，它由D维欧几里得空间中的随机变量x的N次观测组成。我们的目标是将数据集划分为K个类别。现阶段我们假定K的值是给定的。直观上讲，我们会认为由一组数据点构成的一个聚类中，聚类内部点之间的距离应该小于数据点与聚类外部的点之间的距离。于是引入一组$$D$$维向量$$μ_k$$，其中$$k=1,...,K$$，且$$μ_k$$是第$$k$$个聚类的中心。我们的目标是找到数据点分别属于的聚类，以及一组向量$${μ_k}$$，使得每个数据点和与它最近的向量$$μ_k$$之间的距离的平方和最小。

现在，比较方便的做法是定义一些记号来描述数据点的聚类情况。对于每个数据点$$x_n$$，我们引入一组对应的二值指示变量$$r_{nk}∈{0,1}$$，其中$$k=1,...,K$$表示数据点$$x_n$$属于$$K$$个聚类中的哪一个，从而如果数据点$$x_n$$被分配到类别$$k$$，那么$$r_{nk}=1$$，且对于$$j≠k$$，有$$r_{nj}=0$$。这被称为“1-of-K”表示方式。之后我们可以定义一个目标函数，有时被称为失真度量（distortion measure），形式为
<center>$$J = \sum\limits_{n=1}^N\sum\limits_{k=1}^Kr_{nk}\Vert x_n - \mu_k\Vert^2$$</center>
它表示每个数据点与它被分配的向量$$μ_k$$之间的距离的平方和。我们的目标是找到$${r_{nk}}$$,$${μ_k}$$的值，使得$$J$$达到最小值。

我们可以用一种迭代的方法完成这件事，其中每次迭代涉及到两个连续的步骤，分别对应$$r_{nk}$$和$$μ_k$$的最优化。首先，我们为$$μ_k$$选择初始值。然后，在第一阶段，我们关于$$r_{nk}$$最小化$$J$$，保持$$μ_k$$固定。在第二阶段，我们关于$$μ_k$$最小化$$J$$，保持$$r_{nk}$$固定。不断重复这个二阶段优化直到收敛。我们会看到，更新$$r_{nk}$$和更新$$μ_k$$的两个阶段分别对应于EM算法中的E（期望）步骤和M（最大化）步骤。

首先考虑确定$$r_{nk}$$。$$J$$是$$r_{nk}$$的一个线性函数，而且与不同的$$n$$相关的项是独立的，因此我们可以对每个$$n$$分别进行最优化，只要$$k$$的值使$$∥x_n−μ_k∥^2$$最小，我们就令$$r_{nk}$$等于$$1$$。换句话说，我们可以简单地将数据点的聚类设置为最近的聚类中心。

现在考虑$$r_{nk}$$固定时，关于$$μ_k$$的最优化。目标函数$$J$$是$$μ_k$$的一个二次函数，令它关于$$μ_k$$的导数等于$$0$$，即可达到最小值，即
<center>$$\sum\limits_{n=1}^Nr_{nk}(x_n - \mu_k) = 0$$</center>
可以很容易地解出$$μ_k$$，结果为
<center>$$\mu_k = \frac{\sum_nr_{nk}x_n}{\sum_nr_{nk}}$$</center>
这个表达式的分母等于聚类$$k$$中数据点的数量，因此这个结果有一个简单的含义，即令$$μ_k$$等于类别$$k$$的所有数据点的均值。因此，上述步骤被称为$$K$$均值（K-means）算法。

重新为数据点分配聚类的步骤以及重新计算聚类均值的步骤重复进行，直到聚类的分配不改变（或直到迭代次数超过了某个最大值）。由于每个阶段都减小了目标函数$$J$$的值，因此算法的收敛性得到了保证。然而，算法可能收敛到$$J$$的一个局部最小值而不是全局最小值。

在实际应用中，一个更好的初始化步骤是将聚类中心选择为由$$K$$个随机数据点组成的子集。还有一点值得注意的地方，$$K$$均值算法本身经常被用于在EM算法之前初始化高斯混合模型的参数。

$$K$$均值算法的一个值得注意的特征是，在每一次迭代中，每个数据点被分配到一个唯一的聚类中。虽然某些数据点与某个特定的中心$$μ_k$$的距离远远小于与其他中心的距离，但是也存在其他的数据点，位于两个聚类中心的大概中间的位置。在后一种情形中，强行将数据点分配到最近的聚类不是最合适的。我们在下一节会看到，通过使用概率的方法，我们得到了对数据点聚类的“软”分配，它反映了在最合适聚类分配上的不确定性。这个概率形式带来了一些数值计算上的优势。

#### 混合高斯
高斯混合概率分布可以写成高斯分布的线性叠加的形式，即
<center>$$p(x) = \sum\limits_{k=1}^K\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)$$</center>
让我们引入一个$$K$$维二值随机变量$$z$$，这个变量采用了“$$1-of-K$$”表示方法，其中一个特定的元素$$z_k$$等于$$1$$，其余所有的元素等于$$0$$。于是$$z_k$$的值满足$$z_k∈{0,1}$$且$$Σ_kz_k=1$$，并且我们看到根据哪个元素非零，向量$$z$$有$$K$$个可能的状态。我们根据边缘概率分布$$p(z)$$和条件概率分布$$p(x|z)$$定义联合概率分布$$p(x,z)=p(z)p(x|z)$$

...待续
