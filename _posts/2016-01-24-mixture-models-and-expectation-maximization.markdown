---
layout:     post
title:      "K-means聚类与混合模型及EM算法"
subtitle:   ""
date:       2016-01-24 23:30:00
author:     "Beld"
header-img: "img/post-bg-ml.png"
tags:
    - Machine Learning
---

#### Motivation 前言
如果我们定义观测变量和潜在变量的一个联合概率分布，那么对应的观测变量本身的概率分布可以通过求边缘概率的方法得到。这使得观测变量上的复杂的边缘概率分布可以通过观测变量与潜在变量组成的扩展空间上的更加便于计算的联合概率分布来表示。因此，潜在变量的引入使得复杂的概率分布可以由简单的分量组成。

除了提供了一个构建更复杂的概率分布的框架之外，混合模型也可以用于数据聚类。一个非概率的聚类方法就是$$K$$均值算法。之后，我们引入混合概率分布的潜在变量观点，其中离散潜在变量可以被看做将数据点分配到了混合概率分布的具体成分当中。潜在变量模型中寻找最大似然估计的一个一般的方法是期望最大化EM算法。

高斯混合模型广泛应用于数据挖掘、机器学习和统计分析中。在许多应用中，参数由最大似然方法确定，通常会使用EM算法。然而，最大似然方法会有一些巨大的局限性。如果使用变分推断的方法，可以得到一个优雅的贝叶斯处理方式。

#### K－means聚类
首先，我们考虑寻找多维空间中数据点的分组或聚类的问题。假设我们有一个数据集$$\{x_1,...,x_N\}$$，它由$$D$$维欧几里得空间中的随机变量$$x$$的$$N$$次观测组成。我们的目标是将数据集划分为$$K$$个类别。现阶段我们假定$$K$$的值是给定的。直观上讲，我们会认为由一组数据点构成的一个聚类中，聚类内部点之间的距离应该小于数据点与聚类外部的点之间的距离。于是引入一组$$D$$维向量$$μ_k$$，其中$$k=1,...,K$$，且$$μ_k$$是第$$k$$个聚类的中心。我们的目标是找到数据点分别属于的聚类，以及一组向量$${μ_k}$$，使得每个数据点和与它最近的向量$$μ_k$$之间的距离的平方和最小。

现在，比较方便的做法是定义一些记号来描述数据点的聚类情况。对于每个数据点$$x_n$$，我们引入一组对应的二值指示变量$$r_{nk}∈{0,1}$$，其中$$k=1,...,K$$表示数据点$$x_n$$属于$$K$$个聚类中的哪一个，从而如果数据点$$x_n$$被分配到类别$$k$$，那么$$r_{nk}=1$$，且对于$$j≠k$$，有$$r_{nj}=0$$。这被称为“1-of-K”表示方式。之后我们可以定义一个目标函数，有时被称为失真度量（distortion measure），形式为
<center>$$J = \sum\limits_{n=1}^N\sum\limits_{k=1}^Kr_{nk}\Vert x_n - \mu_k\Vert^2$$</center>
它表示每个数据点与它被分配的向量$$μ_k$$之间的距离的平方和。我们的目标是找到$${r_{nk}}$$,$${μ_k}$$的值，使得$$J$$达到最小值。

我们可以用一种迭代的方法完成这件事，其中每次迭代涉及到两个连续的步骤，分别对应$$r_{nk}$$和$$μ_k$$的最优化。首先，我们为$$μ_k$$选择初始值。然后，在第一阶段，我们关于$$r_{nk}$$最小化$$J$$，保持$$μ_k$$固定。在第二阶段，我们关于$$μ_k$$最小化$$J$$，保持$$r_{nk}$$固定。不断重复这个二阶段优化直到收敛。我们会看到，更新$$r_{nk}$$和更新$$μ_k$$的两个阶段分别对应于EM算法中的E（期望）步骤和M（最大化）步骤。

首先考虑确定$$r_{nk}$$。$$J$$是$$r_{nk}$$的一个线性函数，而且与不同的$$n$$相关的项是独立的，因此我们可以对每个$$n$$分别进行最优化，只要$$k$$的值使$$∥x_n−μ_k∥^2$$最小，我们就令$$r_{nk}$$等于$$1$$。换句话说，我们可以简单地将数据点的聚类设置为最近的聚类中心。

现在考虑$$r_{nk}$$固定时，关于$$μ_k$$的最优化。目标函数$$J$$是$$μ_k$$的一个二次函数，令它关于$$μ_k$$的导数等于$$0$$，即可达到最小值，即
<center>$$\sum\limits_{n=1}^Nr_{nk}(x_n - \mu_k) = 0$$</center>
可以很容易地解出$$μ_k$$，结果为
<center>$$\mu_k = \frac{\sum_nr_{nk}x_n}{\sum_nr_{nk}}$$</center>
这个表达式的分母等于聚类$$k$$中数据点的数量，因此这个结果有一个简单的含义，即令$$μ_k$$等于类别$$k$$的所有数据点的均值。因此，上述步骤被称为$$K$$均值（K-means）算法。

重新为数据点分配聚类的步骤以及重新计算聚类均值的步骤重复进行，直到聚类的分配不改变（或直到迭代次数超过了某个最大值）。由于每个阶段都减小了目标函数$$J$$的值，因此算法的收敛性得到了保证。然而，算法可能收敛到$$J$$的一个局部最小值而不是全局最小值。

在实际应用中，一个更好的初始化步骤是将聚类中心选择为由$$K$$个随机数据点组成的子集。还有一点值得注意的地方，$$K$$均值算法本身经常被用于在EM算法之前初始化高斯混合模型的参数。

$$K$$均值算法的一个值得注意的特征是，在每一次迭代中，每个数据点被分配到一个唯一的聚类中。虽然某些数据点与某个特定的中心$$μ_k$$的距离远远小于与其他中心的距离，但是也存在其他的数据点，位于两个聚类中心的大概中间的位置。在后一种情形中，强行将数据点分配到最近的聚类不是最合适的。我们在下一节会看到，通过使用概率的方法，我们得到了对数据点聚类的“软”分配，它反映了在最合适聚类分配上的不确定性。这个概率形式带来了一些数值计算上的优势。

#### 混合高斯模型 GMM
高斯混合概率分布可以写成高斯分布的线性叠加的形式，即
<center>$$p(x) = \sum\limits_{k=1}^K\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)$$</center>
让我们引入一个$$K$$维二值随机变量$$z$$，这个变量采用了“1-of-K”表示方法，其中一个特定的元素$$z_k$$等于$$1$$，其余所有的元素等于$$0$$。于是$$z_k$$的值满足$$z_k∈{0,1}$$且$$Σ_kz_k=1$$，并且我们看到根据哪个元素非零，向量$$z$$有$$K$$个可能的状态。我们根据边缘概率分布$$p(z)$$和条件概率分布$$p(x|z)$$定义联合概率分布$$p(x,z)=p(z)p(x|z)$$。$$z$$的边缘概率分布根据混合系数$$π_k$$进行赋值，即
<center>$$p(z_k = 1) = \pi_k$$</center>
其中参数$${π_k}$$必须满足$$0 \leq \pi_k \leq 1$$以及$$\sum\limits_{k=1}^K\pi_k = 1$$。  
由于$$z$$使用了“1-of-K”表示方法，因此我们也可以将这个概率分布写成
<center>$$p(z) = \prod\limits_{k=1}^K \pi_k^{z_k}$$</center>
给定$$z$$的一个特定的值，$$x$$的条件概率分布是一个高斯分布
<center>$$p(x|z_k = 1) = \mathcal{N}(x|\mu_k,\Sigma_k)$$</center>
也可以写成
<center>$$p(x|z) = \prod\limits_{k=1}^K\mathcal{N}(x|\mu_k,\Sigma_k)^{z_k}$$</center>
联合概率分布为$$p(z)p(x|z)$$，从而x的边缘概率分布可以通过将联合概率分布对所有可能的$$z$$求和的方式得到，即
<center>$$p(x) = \sum\limits_zp(z)p(x|z) = \sum\limits_{k=1}^K\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)$$</center>
所以$$x$$的边缘分布是高斯混合分布。因此对于每个观测数据点$$x_n$$，存在一个对应的潜在变量$$z_n$$。

于是，我们找到了高斯混合分布的一个等价的公式，将潜在变量显式地写出。我们现在能够对联合概率分布$$p(x,z)$$操作，而不是对边缘分布$$p(x)$$操作，这会产生极大的计算上的简化。通过引入期望最大化EM算法，即可看到这一点。

另一个起着重要作用的量是给定$$x$$的条件下，$$z$$的条件概率，用$$γ(z_k)$$表示$$p(z_k = 1)$$，它的值可以使用贝叶斯定理求出
\begin{eqnarray}
\gamma(z_k) \equiv p(z_k = 1|x) &=& \frac{p(z_k = 1)p(x|z_k = 1)}{\sum\limits_{j=1}^Kp(z_j=1)p(x|z_j = 1)} \\
&=& \frac{\pi_k\mathcal{N}(x|\mu_k,\Sigma_k)}{\sum\limits_{j=1}^K\pi_j\mathcal{N}(x|\mu_j,\Sigma_k)}
\end{eqnarray}
我们将$$π_k$$看成$$z_k=1$$的先验概率，将$$γ(z_k)$$看成观测到$$x$$之后，对应的后验概率。

##### 最大似然MLE的问题
假设我们有一个观测的数据集$$\{x_1,...,x_N\}$$，我们希望使用混合高斯模型来对数据进行建模。如果我们假定数据点独立地从概率分布中抽取，这个独立同分布数据集的高斯混合模型的对数似然函数为
<center>$$\ln p(X|\pi,\mu,\Sigma) = \sum\limits_{n=1}^N\ln\left\{\sum\limits_{k=1}^K\pi_k\mathcal{N}(x_n|\mu_k,\Sigma_k)\right\}$$</center>
为了简化起见，我们考虑一个高斯混合模型，它的分量的协方差矩阵为$$\Sigma_k = \sigma_k^2I$$，结论对于一般的协方差矩阵仍然成立。假设混合模型的第$$j$$个分量的均值$$μ_j$$与某个数据点完全相同，即对于某个$$n$$值，$$μ_j=x_n$$。那么，
<center>$$\mathcal{N}(x_n|x_n,\sigma_j^2I) = \frac{1}{(2\pi)^{1/2}}\frac{1}{\sigma_j^D}$$</center>
如果我们考虑极限$$σ_j→0$$，那么我们看到这一项趋于无穷大，因此对数似然函数也会趋于无穷大。因此，对数似然函数的最大化不是一个具有良好定义的问题，因为这种奇异性总会发生在任何一个“退化”到一个具体的数据点上的高斯分量上，这也就是过拟合现象。为什么单一的高斯分布没有出现这个问题呢？如果单一的高斯分布退化到了一个数据点上，那么它总会给由其他数据点产生的似然函数贡献可乘的因子，这些因子会以指数的速度趋于$$0$$，从而使得整体的似然函数趋于零而不是无穷大。我们可以使用合适的启发式方法来避免这种奇异性，例如：如果检测到高斯分量收缩到一个点，那么就将它的均值重新设定为一个随机选择的值，并且重新将它的方差设置为某个较大的值，然后继续最优化。

寻找最大似然解时会产生另一个问题，那就是由于$$k$$个高斯分量的顺序不是固定的，会导致有$$K!$$个等价的解，对应于$$K!$$种将$$K$$个参数集合分配到K个分量上的方式。这个问题被称为可区分（identifiability）问题（Casella and Berger, 2002）。

最大化高斯混合模型的对数似然函数，比单一的高斯分布的情形更加复杂。困难来源于在式中对$$k$$的求和出现在对数计算内部，从而对数函数不再直接作用于高斯分布。如果我们令对数似然函数的导数等于零，那么我们不会得到一个解析解。一种方法是使用基于梯度的优化方法，另一种就是EM算法。

##### 期望最大化算法 EM

首先，让我们写下似然函数的最大值必须满足的条件。令式中$$\ln p(X|\pi, \mu, \Sigma)$$关于高斯分量的均值$$μ_k$$求偏导并令其等于$$0$$（求偏导时先对对数求导，分母为其本身，再对k项高斯求导，为其本身，最后再对指数内求导），最后我们有
<center>$$0 = \sum\limits_{n=1}^K\underbrace{\frac{\pi_k\mathcal{N}(x_n|\mu_k,\Sigma_k)}{\sum_j\pi_j\mathcal{N}(x_n|\mu_j,\Sigma_j)}}_{\gamma(z_{nk})}\Sigma_{k}^{-1}(x_n - \mu_k) $$</center>
后验概率（或称为“责任”）很自然地出现在了等式右侧，我们得到了与K-means相同的形式，只不过在K-means中，$$r_{nk}$$不是$$0$$就是$$1$$，是一种“硬”分配，而这里是“软”分配（soft assignment），反映了在最合适聚类分配上的不确定性。
两侧同时乘以$$Σ_k$$（假设矩阵是非奇异的），整理可得
<center>$$\mu_k = \frac{1}{N_k}\sum\limits_{n=1}^N\gamma(z_{nk})x_n $$</center>
其中我们定义了
<center>$$N_k = \sum\limits_{n=1}^N\gamma(z_{nk}) $$</center>
我们可以将$$N_k$$看做分配到聚类$$k$$的数据点的有效数量。
仔细研究$$\mu_k$$这个解的形式，我们看到第$$k$$个高斯分量的均值$$μ_k$$通过对数据集里所有的数据点求加权平均的方式得到，其中数据点$$x_n$$的权因子由后验概率$$γ(z_{nk})$$给出，而$$γ(z_{nk})$$表示分量$$k$$对生成$$x_n$$的责任。

如果我们令$$\ln p(X | \pi, \mu, \Sigma)$$关于$$Σ_k$$的导数等于$$0$$，然后用一个类似的推理过程，我们有
<center>$$\Sigma_k = \frac{1}{N_k}\sum\limits_{n=1}^N\gamma(z_{nk})(x_n - \mu_k)(x - \mu_k)^T $$</center>
这与一元高斯分布的对应的结果具有相同的函数形式，但是与之前一样，每个数据点都有一个权值，权值等于对应的后验概率，分母为与对应分量相关联的数据点的有效数量。

最后，我们关于混合系数$$π_k$$最大化$$\ln p(X | \pi, \mu, \Sigma)$$。这里我们必须考虑限制条件：混合系数的总和等于1。使用拉格朗日乘数法，最大化下面的量
<center>$$\ln p(X|\pi,\mu,\Sigma) + \lambda\left(\sum\limits_{k=1}^K\pi_k - 1\right) $$</center>
得到
<center>$$0 = \sum_{n=1}^N\frac{\mathcal{N}(x_n|\mu_k,\Sigma_k)}{\sum_{j=1}^K\pi_j\mathcal{N}(x_n|\mu_j,\Sigma_j)} + \lambda $$</center>
注意，如果我们将两边乘以$$π_i$$，然后利用限制条件对$$K$$个$$π_i$$求和，交换左边的求和顺序，我们会发现$$λ=−N$$。使用这个结果消去$$λ$$，可得
<center>$$\pi_k = \frac{N_k}{N}$$</center>
从而第$$k$$个分量的混合系数为那个分量对于解释数据点的“责任”的平均值。

虽然这些结果并没有给出混合模型参数的一个解析解，然而，这些结果确实给出了一个简单的迭代方法来寻找问题的最大似然解。这个迭代过程是EM算法应用于高斯混合模型的一个实例。我们首先为均值、协方差、混合系数选择一个初始值。然后，我们交替进行两个更新，被称为E步骤和M步骤。在期望E步骤中，我们使用参数的当前值计算公式给出的后验概率（也被称为“责任”）。然后，我们将计算出的概率用于最大化M步骤中，重新估计均值、方差和混合系数。注意，在进行这一步骤时，我们首先计算新的均值，然后使用新的均值找到协方差。我们稍后会证明，每次通过E步骤和接下来的M步骤对参数的更新确保了对数似然函数的增大。在实际应用中，当对数似然函数的变化量或者参数的变化量低于某个阈值时，我们就认为算法收敛。

注意，与K均值算法相比，EM算法在达到（近似）收敛之前，经历了更多次的迭代，每次迭代需要更多的计算量。因此，通常运行K均值算法找到高斯混合模型的一个合适的初始化值，接下来使用EM算法进行调节。协方差矩阵可以很方便地初始化为通过K均值算法找到的聚类的样本协方差，混合系数可以被设置为分配到对应类别中的数据点所占的比例。算法必须避免似然函数带来的奇异性，即高斯分量退化到一个具体的数据点。应该强调的是，通常对数似然函数会有多个局部极大值，EM不保证找到这些极大值中最大的一个。

算法总结如下：

1. 初始化均值$$μ_k$$、协方差$$Σ_k$$和混合系数$$π_k$$，计算对数似然函数的初始值。
2. E步骤。使用当前参数值计算“责任”。
    <center>$$\gamma(z_{nk}) = \frac{\pi_k\mathcal{N}(x_n|\mu_k,\Sigma_k)}{\sum\limits_{j=1}^K\pi_j\mathcal{N}(x_n|\mu_j,\Sigma_j)} $$</center>
3. M步骤。使用当前的“责任”重新估计参数。
    <center>$$\mu_k^{new} &=& \frac{1}{N_k}\sum\limits_{n=1}^N\gamma(z_{nk})x_n $$</center>
    <center>$$\Sigma_k^{new} &=& \frac{1}{N_k}\sum\limits_{n=1}^N\gamma(z_{nk})(x_n - \mu_k^{new})(x_n - \mu_k^{new})^T $$</center>
    <center>$$\pi_k^{new} &=& \frac{N_k}{N}$$</center>
    其中
    <center>$$N_k = \sum\limits_{n=1}^N\gamma(z_{nk}) $$</center>
4. 计算对数似然函数
    <center>$$\ln p(X|\mu,\Sigma,\pi) = \sum\limits_{n=1}^N\ln\left\{\sum\limits_{k=1}^K\pi_k\mathcal{N}(x_n|\mu_k,\Sigma_k)\right\} $$</center>
    检查参数或者对数似然函数的收敛性。如果没有满足收敛的准则，则返回第2步。

#### EM的另一种观点
EM算法的目标是找到具有潜在变量的模型的最大似然解。将所有模型参数的集合被记作θ，因此对数似然函数为
<center>$$\ln p(X|\theta) = \ln\left\{\sum\limits_Z p(X,Z|\theta)\right\}$$</center>
上式一个关键的问题是对于潜在变量的求和位于对数的内部。求和式的出现阻止了对数运算直接作用于联合概率分布，使得最大似然解的形式更加复杂。

现在假定对于$$X$$中的每个观测，我们都有潜在变量$$Z$$的对应值。我们将$${X,Z}$$称为完整（complete）数据集，并且我们称实际的观测数据集$$X$$是不完整的（incomplete）。在实际应用中，我们没有完整数据集$${X,Z}$$，只有不完整的数据$$X$$。我们关于潜在变量$$Z$$的取值的知识仅仅来源于后验概率分布$$p(Z|X,θ)$$。由于我们不能使用完整数据的对数似然函数，因此我们反过来考虑在潜在变量的后验概率分布下，它的期望值，这对应于EM算法中的E步骤。在接下来的M步骤中，我们最大化这个期望。如果当前对于参数的估计为$$θ^{old}$$，那么一次连续的E步骤和M步骤会产生一个修正的估计$$θ^{new}$$。算法在初始化时选择了参数$$θ_0$$的某个起始值。

在E步骤中，我们使用当前的参数值$$θ^{old}$$寻找潜在变量的后验概率分布$$p(Z|X,θ^{old})$$。然后，我们使用这个后验概率分布计算完整数据对数似然函数对于一般的参数值θ的期望。这个期望被记作$$Q(θ,θ^{old})$$，由
<center>$$Q(\theta, \theta^{old}) = \sum\limits_Zp(Z|X,\theta^{old})\ln p(X,Z|\theta)$$</center>
给出。在M步骤中，我们通过最大化
<center>$$\theta^{new} = \arg\max Q(\theta, \theta^{old})$$</center>
来确定修正后的参数估计$$θ^{new}$$。注意，在$$Q(θ,θ^{old})$$的定义中，对数操作直接作用于联合概率分布$$p(X,Z|θ)$$，因此根据假设，对应的M步骤的最大化是可以计算的。

一般的EM算法总结如下。最后我们证明，每个EM循环都会增大不完整数据的对数似然函数（除非已经达到局部极大值）。

##### 一般的EM算法
给定观测变量$$X$$和潜在变量$$Z$$上的一个联合概率分布$$p(X,Z|θ)$$，由参数$$θ$$控制，目标是关于$$θ$$最大化似然函数$$p(X|θ)$$。
1. 选择参数θold的一个初始设置。
2. E步骤。计算p(Z|X,θold)。
3. M步骤。计算θnew，由
    <center>$$\theta^{new} = \arg\max_{\theta} Q(\theta, \theta^{old})$$</center>
    给出。其中
    <center>$$Q(\theta, \theta^{old}) = \sum\limits_Zp(Z|X,\theta^{old})\ln p(X,Z|\theta) $$</center>
4. 检查对数似然函数或者参数值的收敛性。如果不满足收敛准则，那么令$$\theta^{old} = \theta^{new}$$。然后回到第2步。

EM算法也可以用来寻找模型的MAP（最大后验概率）解，此时我们定义一个参数上的先验概率分布$$p(θ)$$。在这种情况下，E步骤与最大似然的情形相同，而在M步骤中，需要最大化的量为$$Q(θ,θ^{old})+lnp(θ)$$。
