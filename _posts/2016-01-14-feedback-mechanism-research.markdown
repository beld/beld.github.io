---
layout:     post
title:      "Research on Ratings in Recommender System"
subtitle:   ""
date:       2016-01-14 13:30:00
author:     "Beld"
header-img: "img/post-bg-engineer.jpg"
tags:
    - recommender system
---

#### Feedback mechanism overview

##### Explicit Feedback

Explicit feedback is a process that users express their opinions towards an item using the mechanisms provided by the system. There are several domain independent tools for capturing and analyzing explicit feedback, such as Likert scale (like/dislike, star rating), questionnaire, and review/comment.  
[Characterization of Explicit Feedback in an Online Music Recommendation Service] observed that the rate of providing explicit feedback by a user decreases over time and overall providing explicit feedback has a negative effect on the user’s behavior.   

##### Implicit Feedback

Implicit feedback is generated by system itself through observing and inferring the user’s behaviors. This approach measures user’s taste and interest without seeking for user’s consent. It relies on application dependent tools and methodologies for capturing and interpreting implicit feedback. Modeling user preferences on the basis of implicit feedback has a major limitation: the underlying assumption is that the amount of time that users spend accessing a given content is directly proportional to how much they like it. Therefore, while the explicit feedback can be positive and negative, the implicit feedback is only positive. And implicit feedback tends to be relative where as explicit feedback is absolute.  

##### Comparison of implicit and explicit feedback

 － | Implicit feedback |  Explicit feedback
--|---|--
Accuracy  |  Low |  Hign
Abundance  |  High |  Low
Context-sensitive  |  Yes |  Yes
Expressivity of user preference  |  Positive |  Positive and Negative
Measurement reference  |  Relative |  Absolute
Susceptibility to noise  |  Yes |  Yes
User’s effort  |   Low|  High
Evaluation metrics | Not clear | Clear

##### Hybrid feedback

Hybrid feedback is the combination of both implicit and explicit feedback. This approach utilizes both numerical rating scores and human behaviors in predicting items of interest and taste to the users. However, it is computationally intensive.

#### Rating

##### Mapping opinions to ratings – Design of rating scales

The role of rating scales is crucial in recommender systems where suggestions are generated by predicting ratings for items users are unaware of, based on ratings users explicitly provided for other items. A rating scale should let users to express their opinion easily without too much effort. However, the opinions can be complex. It is commonly accepted that users have different preferences with respect to the rating scales to use for the topic they are evaluating, and that they prefer different rating scales for evaluating different topics.  And user’s rating might not show true opinion of users.

[The Impact of Rating Scales on User’s Rating Behavior] defined rating scales as complex widgets characterized by: i) granularity, i.e. the number of positions on the scale: coarse (e.g., a 3-points scale) or fine (e.g., a 10-points scale); ii) numbering, i.e. the numbers, if any, which can be associated with each position (e.g., 3-points rating scales might be numbered 0,1,2; 1,2,3; or -1,0,+1); iii) visual metaphor, i.e. the visualization form which influences the emotional connotation of each scale: e.g., a smiley face rating scale is a metaphor related to human emotions; a star rating scale is a metaphor which relies heavily on ranking and scoring conventions (e.g., hotel ratings); both can also convey cultural connotations; iv) neutral position, i.e. the presence of an intermediate, neutral point.

[Is Seeing Believing? How Recommender Interfaces Affect Users’ Opinions] conducted a survey and verified that users prefer finer-grained rating scales. They asked users to rank four scales in order of preference, binary (Thumbs up or thumbs down), no-zero (A scale from +3 to -3 with no zero), and half-star scales (A 0.5 to 5 star scale in half star increments), as well as on the original MovieLens scale. Users liked the half-star scale most (average satisfaction of 4.2), followed by the original MovieLens scale (average 3.8), the no-zero scale (3.2), and the binary scale (2.2). However, granularity is not the only factor, or else users would like the no-zero scale better than the original MovieLens scale. It may be that users were more comfortable with the familiar original scale. They also discussed the following two factors affecting ratings, but without general conclusions:
1.	How do different rating scales affect users’ ratings?
2.	Does the rating scale affect prediction accuracy of common collaborative filtering algorithms?  

They also believe that recommender systems are self-correcting: that, if artificially high ratings are given for an item, other users will give true ratings for that item that will cause it to not be recommended any more. However, users of a recommender system are also sensitive to quality. Another remark is that designers should take care that the scale allows users to make meaningful distinctions. For instance, users may not need to distinguish between degrees of badness.


##### Experiments on 5-star and like/dislike rating systems

The goal of these experiments is to measure the most comfortable and easiest way the users use to rate a content explicitly in order to determine which of the two methods of rating is more effective and most used by users.

[An Online Evaluation of Explicit Feedback Mechanisms for Recommender Systems] studied user behavior towards four different explicit feedback mechanisms that are most commonly used in online systems, 5-star rating (static and dynamic) and thumbs up/down (static and dynamic). They integrated these systems into a popular (10,000 visitors a day) cultural events website and monitored the interaction of users.

![four-feedback-mechanisms](/images/2016/01/Picture1.png)

Their results showed that the static 5-star rating mechanism collected the most feedback, closely followed by the dynamic thumbs up/down system. The 5-star systems failed however to produce more accurate feedback than the thumbs systems.

[Social Voting Techniques: A Comparison of the Methods Used for Explicit Feedback in Recommendation Systems] compared the “5 starts” and “Like” and obtained some interesting results:
1.	The users preferred to use the method "5 stars" with 57% of the total, respect to method "Like" that represents the 43% of the total, however this is not a significant difference to assert that method "5 stars" is more used by the users.
2.	The 48% of the contents, have been rated with 5 stars, this means that almost the half of the contents of the recommendation system are very much liked by users or that their rates are usually 5 stars when they like the content.
3.	Men do like the method "Like" and the women do like the method "5 stars".
4.	In the recommendation system when men use the method "5 stars" they prefer to assign a qualification of 4 stars to the contents they like while women prefer to assign a qualification of 5 stars.
5.	Despite the similarity of the evaluation results retrieved from both methods, they believe that the "like" method could be more accurate than the 5-star method.

##### Personalized interfaces or rating scales?

[Towards a Customization of Rating Scales in Adaptive Systems] proposed to allow users to choose the rating scales to use in recommender systems. But recommender systems must be able to deal with ratings expressed by means of heterogeneous scales, mapping them to an internal representation, in order to generate correct recommendations. However, rating scales actually have an influence on user ratings expressed with different rating scales that mathematical normalization is not enough for mapping [The Impact of Rating Scales on User’s Rating Behavior].

##### Rating inconsistencies – Noise problem

Users are inconsistent in giving their feedback, thus introducing an unknown amount of noise that challenges the validity of this assumption.

[Is seeing believing?: how recommender system interfaces affect users’ opinions] carried out an experiment using a rate-rerate procedure with two trials on 212 participants. They selected 40 random movies in the center of the rating scale (i.e. 2,3 or 4 rating) that participants had already rated in the past – months or even years earlier, according to the authors. They reported participants being consistent only 60% of the time.

[I like it... I like it not: Evaluating User Ratings Noise in Recommender Systems]
Analysis of inconsistency and noise in users’ rating is related to the concept of reliability of user tests. Reliability in this context is defined as the ratio of true score variance over the observed score variance. Since true scores are unknown, it is not possible to compute reliability directly. They used the test-retest reliability method to estimate it which a function of the Pearson correlations between the different trials of the same test. However, a different rating over time could be due to either the reliability of the measure and the user’s response or to the fact that the user’s opinion has changed during that period. Therefore, the both effects should be distinguished.  They got four conclusions:

1)	Extreme ratings are more consistent than mild opinions; This finding seems to indicate that recommender algorithms could benefit from giving lower weight or importance to ratings in the middle of the rating scale.  
2)	Users are more consistent when movies with similar ratings are grouped together.  
3)	The learning effect on the setting improves the user’s assessment on whether she has seen the movie, but not the stability of the rating itself.  
4)	Faster user clicking does not yield more inconsistencies.  

##### Burden of rating

On the one hand, the system must collect “enough” ratings from the user in order to learn her/his preferences and improve the accuracy of recommendations. On the other hand, gathering more ratings adds a burden on the user, which may negatively affect the user experience.

[User Effort vs. Accuracy in Rating-based Elicitation] conducted three experiments to measure which of the two contrasting forces influenced by the number of collected ratings – recommendations relevance and burden of the rating process – has stronger effects on the perceived quality of the user experience. They found that an increased user effort during the sign-up process negatively affects the perceived quality of user interaction, in terms of global satisfaction, if the extra burden is not compensated by an increased utility (i.e., improved relevance of recommendations). Users were little rewarded by useful recommendations, regardless of the higher number of ratings, and they were more susceptible to feel the additional burden.

As the relevance of recommendations does not increase indefinitely with the profile length, there should be a maximum number of ratings that can be elicited from a new user without having the negative force induced by increased burden overcome the positive force of relevance. From a design perspective, their findings suggest that the optimal number of ratings in the movie domain is between 5 and 20 ratings (more likely 10 ratings): it is within this range that the contrasting forces induced by profile length achieve a better balance, from a user interaction quality perspective.

##### ‘High Ratings/Low Ratings’ problem

Explicit feedback tends to concentrate on either side of the rating scale, as users are more likely to express their preferences if they feel strongly for or against an item [I like it... I like it not: Evaluating User Ratings Noise in Recommender Systems].

[Graph-Based Collaborative Filtering Using Rating Nodes- A Solution to the High Ratings/Low Ratings Problem] proposes a novel graph representation scheme in which item ratings are represented using multiple nodes, allowing flow of information through both low-rating and high-rating connections. There is an improvement of up to 15 % in precision and recall.
