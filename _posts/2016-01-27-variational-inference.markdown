---
layout:     post
title:      "Variational Inference 变分推断"
subtitle:   ""
date:       2016-01-27 12:30:00
author:     "Beld"
header-img: "img/post-bg-ml.png"
tags:
    - Machine Learning
---

#### Motivation 前言
在概率模型的应用中，一个核心任务是在给定观测数据变量$$X$$的条件下，计算潜在变量Z的后验概率分布$$p(Z|X)$$，以及计算关于这个后验概率分布的期望。模型可能也包含某些确定性参数，我们现在不考虑它。模型也可能是一个纯粹的贝叶斯模型，其中任何未知的参数都有一个先验概率分布，并且被整合到了潜在变量集合中，记作向量$$Z$$。对于实际应用中的许多模型来说，计算后验概率分布或者计算关于这个后验概率分布的期望是不可行的。这可能是由于潜在空间的维度太高，以至于无法直接计算，或者由于后验概率分布的形式特别复杂，从而期望无法解析地计算。

在这种情况下，我们需要借助近似方法。近似方法大体分为随机近似和确定性近似两大类。随机方法，例就是如马尔科夫链蒙特卡罗MCMC的采样方法，这些方法通常具有这样的性质：给定无限多的计算资源，它们可以生成精确的结果，近似的来源是使用了有限的处理时间。在实际应用中，取样方法需要的计算量会相当大，经常将这些方法的应用限制在了小规模的问题中。并且，判断一种取样方法是否生成了服从所需的概率分布的独立样本是很困难的。另一种就是确定性近似方法。有些方法对于大规模的数据很适用。这些方法基于对后验概率分布的解析近似，例如通过假设后验概率分布可以通过一种特定的方式分解，或者假设后验概率分布有一个具体的参数形式，例如高斯分布。对于这种情况，这些方法永远无法生成精确的解，因此这些方法的优点和缺点与取样方法是互补的。

#### Functional 泛函
我们可以将函数想象为一个映射。这个映射以一个变量的值作为输入，返回函数值作为输出。函数的导数描述了当输入变量有一个无限小的变化时，输出值如何变化。类似地，我们可以将泛函（functional）作为一个映射，它以一个函数作为输入，返回泛函的值作为输出。一个例子是熵$$H[p]$$，它的输入是一个概率分布$$p(x)$$，返回下面的量
<center>$$H[p] = - \int p(x)\ln p(x) dx$$</center>
作为输出。我们可以引入泛函的导数（functional derivative）的概念，它表达了输入函数产生无穷小的改变时，泛函的值的变化情况。

许多问题可以表示为最优化问题Optimization，其中需要最优化的量是一个泛函，研究所有可能的输入函数，找到最大化或者最小化泛函的函数就是问题的解。虽然变分方法本质上没有任何近似的东西，但是它们通常会被用于寻找近似解。寻找近似解的过程可以这样完成：限制需要最优化算法搜索的函数的范围，例如只考虑二次函数，或考虑由固定的基函数线性组合而成的函数，其中只有线性组合的系数可以发生变化。

在EM算法中，我们的概率模型确定了联合概率分布$$p(X,Z)$$，我们的目标是找到对后验概率分布$$p(Z|X)$$以及模型证据$$p(X)$$的近似。不过，现在把所有潜在变量和参数组成的集合记作$$Z$$，因为参数现在是随机变量，被整合到了$$Z$$中，所以参数向量$$θ$$不再出现。将对数边缘概率分解，即
<center>$$\ln p(x) = L(q) + KL(q \Vert p) $$</center>
其中我们定义了
<center>$$L(q) = \int q(Z)\ln\frac{p(X,Z)}{q(Z)}dZ $$</center>
<center>$$KL(q \Vert p) = - \int q(Z)\ln\frac{p(Z|X)}{q(Z)}dZ $$</center>
因为主要考虑连续变量，这个分解的公式中使用了积分而不是求和。离散变量只需根据需要把积分替换为求和即可。

其实，KL散度就是一个泛函，它表示了用一个分布q去近似另一个未知分布p时平均额外增加的信息量，用来衡量分布之间的相似性，所以也叫做相对熵relative entropy：
<center>$$KL(p\Vert q)=-\int  p(x)\ln { q } (x)dx-(- \int p(x)\ln p(x)dx )＝-\int  p(x)\ln { \frac { q(x) }{ p(x) }  } dx$$</center>

同样的，我们可以通过关于概率分布$$q(Z)$$的最优化来使下界$$L(q)$$达到最大值，这等价于最小化$$KL$$散度。如果我们允许任意选择$$q(Z)$$，那么下界的最大值出现在KL散度等于零的时刻，此时q(Z)等于后验概率分布p(Z|X)。但是，这个后验通常都是不可处理的，我们要充分限制$$q(Z)$$使得这个范围中的所有概率分布都是可以处理的，然后寻找这个范围中使得KL散度达到最小值的概率分布。同时，还要使得这个范围充分大、充分灵活，没有“过拟合”现象。

#### Factorized Distributions 分解分布
假设我们将$$Z$$的元素划分成若干个互不相交的组，记作$$Z_i$$，其中$$i=1,...,M$。然后，我们假定$$q$$分布关于这些分组可以进行分解，即
q(Z) = \prod\limits_{i=1}^Mq_i(Z_i) \tag{10.5}
需要强调的是，我们关于概率分布没有做更多的假设，我们没有限制各个因子qi(Zi)的函数形式。变分推断的这个分解的形式对应于物理学中的一个近似框架，叫做平均场理论（mean field theory）。

在所有具有上面形式的概率分布$$q(Z)$$中，我们寻找下界$$L(q)$$最大的概率分布。于是，我们希望对$$L(q)$$关于所有的概率分布$$q_i(Z_i)$$（简记作$$q_j$$）进行一个自由形式的（变分）最优化。通过关于每个因子进行最优化来完成整体的最优化过程。首先分离出依赖于一个因子qj的项，这样我们就有
\begin{eqnarray}
L(q) &=& \int\prod\limits_iq_i\left\{\ln p(X,Z) - \sum\limits_i\ln q_i\right\}dZ \\
&=& \int q_j\left\{\int \ln p(X,Z)\prod\limits_{i \neq j}q_i dZ_i\right\}dZ_j - \int q_j\ln q_jdZ_j + const \\
&=& \int q_j\ln\tilde{p}(X,Z_j)dZ_j - \int q_j\ln q_j dZ_j + const
\end{eqnarray}
其中，我们定义了一个新的概率分布p~(X,Zj)，形式为
\ln\tilde{p}(X,Z_j) = \mathbb{E}_{i \neq j}[\ln p(X,Z)] + const \tag{10.7}
