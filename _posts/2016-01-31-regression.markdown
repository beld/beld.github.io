---
layout:     post
title:      "贝叶斯线性回归：最大似然，最大后验与贝叶斯推断"
subtitle:   ""
date:       2016-01-31 16:00:00
author:     "Beld"
header-img: "img/post-bg-ml.png"
tags:
    - Machine Learning
---

#### 总结

回归是用来寻找给定数据集的数学模型的一种方法，给定包含观测量与对应的目标变量的训练数据集，目标是预测出新的观测值的目标变量。多项式是线性回归模型这一大类函数的一个特殊例子。最简单的线性回归模型形式也是输入变量的线性函数。但是，通过将输入变量的非线性函数进行线性组合，可以得到一类被称为基函数（basis function）的更加有用的函数。这样的参数的线性函数模型，使其具有一些简单的分析性质，且关于输入变量是非线性的。

多项式拟合采用多项式基函数，它们是输入变量的全局函数，造成了局限，可以切分输入空间，采用样条函数（spline function）。其他基函数有高斯基函数和sigmoid基函数，等价的，我们可以使用“tanh”函数。另一种可能的基函数选择是傅里叶基函数，它引出正弦函数展开，如小波（wavelet）。

可以通过最小化误差函数使多项式函数拟合数据集。一种广泛使用的简单的选择是把每一个数据的目标值与预测值的平方差的和作为误差函数。多项式的阶数要合适，过高会导致过拟合（overfitting）。降低了模型的泛化能力。大致的概括，数据点的数量不应该小于模型的可调节参数的数量的若干倍(比如5或10)。

正则化(regularization)是一种经常用来控制过拟现象的技术，涉及到给误差函数增加一个惩罚项,使得系数不会取很大的值。采用所有系数的平方和作为惩罚项是一种最简单的形式。因为这种方法减小了系数的值，意思在统计学文献中它被称为收缩(shrinkage)方法。二次正则项的一种特殊情况被称为山脊回归(ridge regression)。在神经网络中,这种方法被叫做权值衰减(weight decay)。

如果数据是受随机噪声干扰的，这导致对于观测量对应的目标量具有不确定性。下面我们从概率的角度来看待回归。假定噪声服从高斯分布，对于单个观测量，这个不确定性的概率就可以表示成以多项式拟合这个观测量的结果为均值的高斯分布。假定数据集的数据是独立同分布的，那么就可以写出似然函数，即多个高斯的连乘，然后最大化似然函数的对数。我们就会发现，在高斯噪音条件下，最大似然结果等价于最小化平方和误差函数。

现在让我们朝着贝叶斯的方法前进一步。过拟合问题可以作为最大似然的一个通用属性来理解，采用贝叶斯方法可以避免过拟问题。在模型参数上引入先验分布，简单起见（与似然函数同分布），我们考虑高斯分布。使用贝叶斯定理，参数的后验分布，正比于先验分布和似然函数的乘积。然后得到最可能的参数值，即最大化后验分布（maximum posterior）或简写为MAP。我们还可以发现，最大化后验概率等价于最小化正则化的平方和误差函数。

最大似然解涉及一次处理整个数据集，对于大规模数据集来说计算量相当大。当数据集相当大的时候，值得使用一种每次考虑一个数据点，然后更新模型参数的被称为在线算法的顺序算法。

不管是似然函数还是后验分布，都是关于参数的函数，最大化之后我们得到的都是基于已知数据的最优的参数估计。然而我们往往对预测新的目标值更感兴趣。所以确定参数后，由于现在有了概率模型，可以使用预测分布（predictive distribution）来表达目标值的概率分布，来代替一个简单的点估计，这就是贝叶斯推断：新数据的似然函数乘以之前的后验作为新数据的先验，然后对所有参数值进行积分，就得到了预测分布，这样的边缘化是使用贝叶斯方法的核心。

贝叶斯学习的顺序本质，就是当新数据点被观测到的时候，当前的后验分布变成了先验分布。

我们可以通过随机梯度下降（stochastic gradient descent）在后面也被称为顺序梯度下降（sequential gradient descent）来得到顺序学习算法。
