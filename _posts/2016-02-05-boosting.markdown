---
layout:     post
title:      "Boosting"
subtitle:   ""
date:       2016-02-05 24:00:00
author:     "Beld"
header-img: "img/post-bg-ml.png"
tags:
    - Machine Learning
---

#### AdaBoost
Boosting算法系列的起源来自于PAC学习模型中弱学习算法和强学习算法的等价性问题，即任意给定仅比随机猜测略好的弱学习算法，是否可以将其提升为强学习算法？如果二者等价，那么只需找到一个比随机猜测略好的弱学习算法就可以将其提升为强学习算法，而不必寻找很难获得的强学习算法。Schapire在1996年提出一个有效的算法真正实现了这个夙愿，它的名字叫AdaBoost。AdaBoost把多个不同的决策树用一种非随机的方式组合起来，表现出惊人的性能！第一，把决策树的准确率大大提高，可以与SVM媲美。第二，速度快，且基本不用调参数。第三，几乎不Overfitting。在Boosting算法产生之前，还出现过两种比较重要的将多个分类器整合为一个分类器的方法，即boostrapping方法和bagging方法。

AdaBoost是一种迭代算法，其核心思想是针对同一个训练集训练不同的基分类器，即弱分类器，然后把这些弱分类器集合起来，构造一个更强的最终分类器。基分类器是顺序训练的，每个基分类器使用数据集的一个加权形式进行训练，每个数据点被赋予了一个关联的权值参数$$w_n$$，对于所有的数据点，它都被初始化为$$1/N$$。其中与每个数据点相关联的权系数依赖于前一个分类器的表现。被一个基分类器错误分类的点在训练序列中的下一个分类器时会被赋予更高的权重。一旦所有的分类器都训练完毕，那么它们的预测就会通过加权投票的方法进行组合，如图所示。
![](https://mqshen.gitbooks.io/prml/content/Chapter14/images/14_1.png)

AdaBoost算法的精确形式叙述如下：

- 初始化数据加权系数$${w_n}$$，方法是对$$n=1,...,N$$，令$$w^{(1)}_n=1/N$$。
- 对于$$m=1,...,M$$：
    - 使用训练数据得到这一步中的分类器$$y_m(x)$$，因为此步中的$${w^{(m)}_n}$$已知，最小化关于$$y_m(x)$$的加权误差函数：
        <center>$$J_m = \sum\limits_{n=1}^Nw_n^{(m)}I(y_m(x_n) \neq t_n) $$</center>
        就可以得到$$y_m(x)$$。其中$$I(y_m(x_n)≠t_n）$$是一个指示函数，当$$y_m(x_n)≠t_n$$时，值为$$1$$，其他情况下值为$$0$$。
    - 计算
        <center>$$\epsilon_m = \frac{\sum\limits_{n=1}^N w_n^{(m)}I(y_m(x_n) \neq t_n)}{\sum\limits_{n=1}^N w_n^{(m)}} $$</center>
        然后计算
        <center>$$\alpha_m = \ln\left\{\frac{1-\epsilon_m}{\epsilon_m}\right\} $$</center>
    - 更新数据权系数，得到下一步的权系数
        <center>$$w_n^{(m+1)} = w_n^{(m)}exp\{\alpha_mI(y_m(x_n) \neq t_n)\} $$</center>
- 使用最终的模型进行预测，形式为
<center>$$Y_M(x) = sign\left(\sum\limits_{m=1}^M\alpha_my_m(x)\right) $$</center>

我们看到第一个基分类器$$y_1(x)$$使用全部相等的加权系数$$w^{(1)}_n$$进行训练，因此它对应于训练单一的分类器的通常的步骤。我们看到在后续的迭代过程中，权系数$$w^{(m)}_n$$对于误分类的数据点会增大，对于正确分类的数据点不改变。因此后续的分类器就会更关注那些被前一个分类器错误分类的数据点。$$ϵ_m$$表示每个基分类器在数据集上的加权错误率。于是权系数$$α_m$$会为更准确的分类器赋予更高的权值。

AdaBoost算法如图所示：
![](https://mqshen.gitbooks.io/prml/content/Chapter14/images/14_2.png)
基分类器由作用于某个轴的简单的阈值组成。每张图给出了目前训练的基分类器的数量$$m$$，以及最近的基分类器的决策边界（黑色虚线）和组合的决策边界（绿色实线）。每个数据点用圆圈表示，它的半径表示在训练最近添加的基分类器时数据点的权值。因此，例如，我们看到被$$m=1$$的分类器误分类的点在训练$$m=2$$的分类器时被赋予了更高的权值。

这里，每个基分类器由一个输入变量的阈值组成。这个简单的分类器对应于一种被称为“决策树桩”(Decision Stumps)的决策树形式，即一个具有单结点的决策树，是一种非常简单的弱分类器。每个基分类器根据一个输入特征是否超过某个阈值对输入进行分类，因此仅仅使用一个与一个坐标轴垂直的线性决策面将空间划分为两个区域。

#### 最小化指数误差
上面提到的Boosting算法的理论根据是什么？下面我们来推导指数误差函数的顺序最小化的过程：

考虑下面定义的指数误差函数
<center>$$E = \sum\limits_{n=1}^Nexp\{-t_nf_m(x_n)\}$$</center>
其中$$f_m(x)$$是一个根据基分类器$$y_l(x)$$的线性组合定义的分类器，形式为
<center>$$f_m(x) = \frac{1}{2}\sum\limits_{l=1}^m\alpha_ly_l(x) $$</center>
$$t_n ∈ \{ −1,1 \}$$是训练集目标值。我们的目标是关于权系数$$α_l$$和基分类器$$y_l(x)$$最小化$$E$$。

然而，我们不进行误差函数的全局最小化，而是进行顺序最小化。假设基分类器$$y_1(x),...,y_{m−1}(x)$$以及它们的系数$$α_1,...,α_{m−1}$$已知且固定，因此我们只关于$$α_m$$和$$y_m(x)$$进行最小化。

分离出基分类器$$y_m(x)$$的贡献，我们可以将误差函数写成

$$
E &=& \sum\limits_{n=1}^N exp\{-t_nf_{m-1}(x_n) - \frac{1}{2}t_n\alpha_my_m(x_n)\} \\
&=& \sum\limits_{n=1}^N w_n^{(m)} exp\{-\frac{1}{2}t_n\alpha_my_m(x_n)\}
$$

其中，系数$$w_n^{(m)} = exp\{−t_nf_{m-1}(x_n)\}$$，因为只是关于$$α_m$$和$$y_m(x)$$的函数。如果我们将被$$y_m(x)$$正确分类的数据点的集合记作$$T_m$$，并且将剩余的误分类的点记作$$M_m$$，那么我们可以将误差函数写成下面的形式

$$
\begin{eqnarray}
E &=& e^{-\alpha_m / 2}\sum\limits_{n \in T_m}w_n^{(m)} + e^{\alpha_m / 2}\sum\limits_{n \in M_m}w_n^{(m)} \\
&=& (e^{\alpha_m / 2} - e^{-\alpha_m / 2})\sum\limits_{n=1}^Nw_n^{(m)}I(y_m(x_n) \neq t_n) + e^{-\alpha_m / 2}\sum\limits_{n=1}^Nw_n^{(m)}
\end{eqnarray}
$$

在关于$$y_m(x)$$进行最小化时，将正确分类的集合转化为全部的减去错误分类的，第二项就可以被看作是常数，就可以将整式转化为对错误分类集合进行最小化，就得到了AdaBoost算法中的关于$$y_m(x)$$的目标函数。类似地，关于$$α_m$$最小化，就得到了关于$$α_m$$的最小化，就可以得到$$α_m$$。

找到$$α_m$$和$$y_m(x)$$之后，数据点的权值使用下面的公式进行更新
<center>$$w_n^{(m+1)} = w_n^{(m)} exp\left\{-\frac{1}{2}t_n\alpha_my_m(x_n)\right\} $$</center>
因为
<center>$$t_ny_m(x_n) = 1 - 2I(y_m(x_n) \neq t_n) $$</center>
我们看到在下一次迭代中，权值$$w_n^{(m)}$$的更新为
<center>$$w_n^{(m+1)} = w_n^{(m)} exp\left(-\frac{\alpha_m}{2}\right)exp\{\alpha_mI(y_m(x_n) \neq t_n)\} $$</center>
